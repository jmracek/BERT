.version 6.4
.target sm_70
.address_size 64

/* mmultShmemTiles
*
* This function is a PTX wrapper around the 4 mma.sync instructions required to compute a 32x32 block of A * B + C.  
* It was an interesting attempt to combine all four intertwined MMA operations in a single device function, but in
* practice I'm seeing a lot of register spilling that I don't know how to deal with.  This resulted in the second
* device function found here, mma_mult_acc, which performs only a single instruction at a time.
*
* ARGS:
*   ptr_a:  Pointer to the 128-bits of shared memory containing the 8 fp16 elements of the A matrix
*           needed by the current thread in performing the 4 mma.sync operations.
*   ptr_b:  Pointer to the 128-bits of shared memory containing the 8 fp16 elements of the B matrix
*           needed by the current thread in performing the 4 mma.sync operations
*   acc:    This is an array in local memory containing the 8 elements of the accumulator, for each
*           of the 4 mma.sync operations.
*/
// Probably need to set alignment by .ptr.global.align 32
.visible .func mmultShmemTiles( 
    .param .u64 ptr_a, 
    .param .u64 ptr_b, 
    .param .u64 mma0_acc_03,
    .param .u64 mma0_acc_47,
    .param .u64 mma1_acc_03,
    .param .u64 mma1_acc_47,
    .param .u64 mma2_acc_03,
    .param .u64 mma2_acc_47,
    .param .u64 mma3_acc_03,
    .param .u64 mma3_acc_47
) {
    // Load the A, B pointers into registers
    .reg .u64 rptr_a, rptr_b;
    .reg .u64 shmem_ptr_a, shmem_ptr_b;
    ld.param.u64 rptr_a, [ptr_a];
    ld.param.u64 rptr_b, [ptr_b];
    cvta.to.shared.u64 shmem_ptr_a, rptr_a;
    cvta.to.shared.u64 shmem_ptr_b, rptr_b;

    // Declare all the registers we need for this computation
    .reg .f16x2     %Ra<4>, %Rb<4>;
    .reg .f32       %Rc<8>, %Rd<8>, %Re<8>, %Rf<8>;
    
    // Load A and B matrices from SHMEM into registers
    ld.shared.v4.b32 {%Ra0, %Ra1, %Ra2, %Ra3}, [shmem_ptr_a];
    ld.shared.v4.b32 {%Rb0, %Rb1, %Rb2, %Rb3}, [shmem_ptr_b];
    
    .reg .u64 %qptr<8>;
    ld.param.u64 %qptr0, [mma0_acc_03];
    ld.param.u64 %qptr1, [mma0_acc_47];
    ld.param.u64 %qptr2, [mma1_acc_03];
    ld.param.u64 %qptr3, [mma1_acc_47];
    ld.param.u64 %qptr4, [mma2_acc_03];
    ld.param.u64 %qptr5, [mma2_acc_47];
    ld.param.u64 %qptr6, [mma3_acc_03];
    ld.param.u64 %qptr7, [mma3_acc_47];
    
    // Load all accumulators into registers
    ld.v4.f32 {%Rc0, %Rc1, %Rc2, %Rc3}, [%qptr0];
    ld.v4.f32 {%Rc4, %Rc5, %Rc6, %Rc7}, [%qptr1]; 
    ld.v4.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, [%qptr2];
    ld.v4.f32 {%Rd4, %Rd5, %Rd6, %Rd7}, [%qptr3];
    ld.v4.f32 {%Re0, %Re1, %Re2, %Re3}, [%qptr4];
    ld.v4.f32 {%Re4, %Re5, %Re6, %Re7}, [%qptr5];
    ld.v4.f32 {%Rf0, %Rf1, %Rf2, %Rf3}, [%qptr6];
    ld.v4.f32 {%Rf4, %Rf5, %Rf6, %Rf7}, [%qptr7];
    
    // Perform the four mma.sync operations for the first half of the shmem tile
    // * -
    // - -
    mma.sync.aligned.m8n8k4.col.row.f32.f16.f16.f32 
        {%Rc0, %Rc1, %Rc2, %Rc3, %Rc4, %Rc5, %Rc6, %Rc7},
        {%Ra0, %Ra1},
        {%Rb0, %Rb1},
        {%Rc0, %Rc1, %Rc2, %Rc3, %Rc4, %Rc5, %Rc6, %Rc7};

    // - *
    // - -
    mma.sync.aligned.m8n8k4.col.row.f32.f16.f16.f32 
        {%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7},
        {%Ra0, %Ra1},
        {%Rb2, %Rb3},
        {%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7};
    
    // - -
    // * -
    mma.sync.aligned.m8n8k4.col.row.f32.f16.f16.f32 
        {%Re0, %Re1, %Re2, %Re3, %Re4, %Re5, %Re6, %Re7},
        {%Ra2, %Ra3},
        {%Rb0, %Rb1},
        {%Re0, %Re1, %Re2, %Re3, %Re4, %Re5, %Re6, %Re7};
    
    // - -
    // - *
    mma.sync.aligned.m8n8k4.col.row.f32.f16.f16.f32 
        {%Rf0, %Rf1, %Rf2, %Rf3, %Rf4, %Rf5, %Rf6, %Rf7},
        {%Ra2, %Ra3},
        {%Rb2, %Rb3},
        {%Rf0, %Rf1, %Rf2, %Rf3, %Rf4, %Rf5, %Rf6, %Rf7};

    // Now update each of the accumulators 
    st.v4.f32 [%qptr0], {%Rc0, %Rc1, %Rc2, %Rc3};
    st.v4.f32 [%qptr1], {%Rc4, %Rc5, %Rc6, %Rc7};
    st.v4.f32 [%qptr2], {%Rd0, %Rd1, %Rd2, %Rd3};
    st.v4.f32 [%qptr3], {%Rd4, %Rd5, %Rd6, %Rd7};
    st.v4.f32 [%qptr4], {%Re0, %Re1, %Re2, %Re3};
    st.v4.f32 [%qptr5], {%Re4, %Re5, %Re6, %Re7};
    st.v4.f32 [%qptr6], {%Rf0, %Rf1, %Rf2, %Rf3};
    st.v4.f32 [%qptr7], {%Rf4, %Rf5, %Rf6, %Rf7};

    ret;
}


/*
This device function is similar to the previous one, but we only perform a single MMA and call it four times.
*/
.visible .func mma_mult_acc( .param .u64 a, .param .u64 b, .param .u64 c_lo, .param .u64 c_hi ) {
    // Registers for mma operation
    .reg .f16x2     %Ra<2>, %Rb<2>;
    .reg .f32       %Rc<8>;
    
    .reg .u64 %qptr<2>;
    // Load a and b values into registers
    ld.param.u64 %qptr0, [a];
    ld.param.u64 %qptr1, [b];
    ld.v2.b32 {%Ra0, %Ra1}, [%qptr0];
    ld.v2.b32 {%Rb0, %Rb1}, [%qptr1];

    // Load c values into registers
    ld.param.u64 %qptr0, [c_lo];
    ld.param.u64 %qptr1, [c_hi];
    ld.v4.f32 {%Rc0, %Rc1, %Rc2, %Rc3}, [%qptr0];
    ld.v4.f32 {%Rc4, %Rc5, %Rc6, %Rc7}, [%qptr1]; 

    mma.sync.aligned.m8n8k4.col.row.f32.f16.f16.f32 
        {%Rc0, %Rc1, %Rc2, %Rc3, %Rc4, %Rc5, %Rc6, %Rc7},
        {%Ra0, %Ra1},
        {%Rb0, %Rb1},
        {%Rc0, %Rc1, %Rc2, %Rc3, %Rc4, %Rc5, %Rc6, %Rc7};

    st.v4.f32 [%qptr0], {%Rc0, %Rc1, %Rc2, %Rc3};
    st.v4.f32 [%qptr1], {%Rc4, %Rc5, %Rc6, %Rc7};
    
    ret;
}
